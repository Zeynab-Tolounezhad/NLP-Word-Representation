# NLP-Word-Representation
Word representations are a way to convert words into a numerical format that can be used by machine learning models to process and analyze text. There are several approaches to creating word representations, ranging from simple methods to more complex and sophisticated. Two of the methods under review in this repository are Method word2vec and Method doc2vec.
# word2vec
Word2vec is a technique in natural language processing for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words.
Word2vec can utilize either of two model architectures:
 Continuous Bag of Words (CBOW) , Skip-gram
# doc2vec
Doc2Vec is also called a Paragraph Vector a popular technique in Natural Language Processing that enables the representation of documents as vectors. The vectors are learned in such a way that similar documents are mapped to nearby points in the vector space. This enables us to compare documents based on their vector representation and perform tasks such as document classification, clustering, and similarity analysis.
