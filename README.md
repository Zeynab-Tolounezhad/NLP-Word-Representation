# NLP-Word-Representation
Word representations are a way to convert words into a numerical format that can be used by machine learning models to process and analyze text. There are several approaches to creating word representations, ranging from simple methods to more complex and sophisticated. Two of the methods under review in this repository are Method word2vec and Method doc2vec.
# word2vec
Word2vec is a technique in natural language processing for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words.
Word2vec can utilize either of two model architectures:
 Continuous Bag of Words (CBOW) , Skip-gram
# doc2vec
